Learn About Data Cloud Core Capabilities  - ~2 hrs 10 mins . 9 Steps
Note- Explore Data Cloud features, use cases, and best practices.

username epic.f0871736144509279@orgfarm.th

 1. unlock your data with data cloud - Complete this trail to gain the skills you need to become proficient with Data Cloud.
 2. Salesforce Data Cloud Quick look - Discover Data Cloud, the real-time platform for customer magic.
 3. Data Cloud use cases - Get the most out of Data Cloud for Sales, Service, Marketing, and more.
 4. Data Cloud powered experiences - Use Data Cloud to create personalized, real-time experiences across Salesforce and beyond.
 5. Data spaces in data cloud: quick look - Learn the benefits of using data spaces in Data Cloud.
 6. Data cloud basics for marketers - Discover the customer data platform for marketers and prepare for implementation.
 7. Data cloud for commerece quick look - Learn how to personalize the shopper experience with Data Cloud.
 8. Ethical Data Use in Personalization - Apply ethical use best practices to deliver responsible personalization solutions.
 9. Knowledge Check: Data Cloud Core Capabilities - Check your understanding Data Cloud core capabilities by completing this knowledge check.



Set Up and Administer Data Cloud
Note- Learn about the setup and administration tools for Data Cloud.

1. Data Cloud Setup - Set up your account and connect data sources to support your company’s business needs.
2. Watch: Data Cloud for Admins - Explore special considerations for setting up Data Cloud. Get tips for packaging, managing, and troubleshooting your Data Cloud instance.
3. Create a Data Stream in Data Cloud - Connect your Salesforce data in Data Cloud.
4. Packaging and Data Kits in Data Cloud - Learn how to create and package components in Data Cloud.
5. Data Cloud Solutions on AppExchange: Quick Look - Expand the power of Salesforce with Data Cloud solutions on AppExchange.
6. Set Up and Administer Data Cloud - Knowledge check

Connect and Model Your Data - ~2 hrs 11 mins • 6 Steps
Note - Connect various data sources and map your data.

1. Ingestion and Modeling in Data Cloud - Connect your data sources and define their relationship in Data Cloud.
2. Watch: Data Ingestion and Mapping - Prepare and organize data for mapping in Data Cloud.
3. Watch: Demo: Ingestion - Explore data streams and available ingestion methods, ingest from Salesforce Sales Bundle, and map contacts from CRM.
4. Watch: Demo: Mapped Data Model Object Relationships - Explore graph view for the data model, orphan objects, and create relationships between Data Model Objects (DMOs).
5. Customer 360 Data Model for Data Cloud - Explore and use data model objects in Data Cloud.
6. Streaming Data Transforms in Data Cloud: Quick Look - Enable customers to clean and prepare data as it enters the system in real time.
7. Batch Data Transforms in Data Cloud: Quick Look - Create and schedule complex data transformations using batch data transforms.
8. Knowledge Check: Connect and Model Your Data - Check your understanding of data connection and modeling by completing this knowledge check.


Resolve Identity  ~1 hr 19 mins • 4 Steps
Note- Explore the identity resolution process in Data Cloud.

1. Data and Identity in Data Cloud - Learn the fundamentals of data mapping, identity resolution, and unified records.
2. Watch: Identity Resolution - Learn about identity resolution and Unified Profiles.
3. Quick Start: Create an Identity Resolution Ruleset - Use a ruleset to guide how Data Cloud unifies your customer profile data.
4. Resolve Identity - Knowledge check

Segment and Gain Insight    - ~3 hrs 45 mins • 9 Steps
Note - Configure segmentation and get insights from your data.

1. Quick Start: Enhance Data with Insights - Set up calculated insights with data from Data Cloud.
2. Quick Start: Create a Data Cloud Segment - Learn how to create and activate a segment in Data Cloud.
3. Segmentation and Activation - Create, filter, and activate marketing segments with Data Cloud.
4. Quick Start: Enhance Data with Insights - Set up calculated insights with data from Data Cloud.
5. Data Cloud Insights - Use your data effectively by creating calculated and real-time insights.
6. Data Cloud Insights Using SQL - Use SQL to create calculated and streaming insights in Data Cloud.
7. Insights Builder in Data Cloud - Use Insights Builder to create insights using a drag-and-drop visual builder.
8. Connect Data Cloud to Copilot and Prompt Builder - Combine harmonized data and generative AI to create powerful interactions and automations.
9. Knowledge Check: Segment and Gain Insight - Check your understanding of segmentation and insights in Data Cloud by completing this knowledge check.

Act on Data --- ~38 mins • 4 Steps
Note- Learn how to use activations and troubleshoot common issues.

1. Watch: Act on Data - Learn about activations and their use cases. Use related attributes, analyze timing dependencies, and troubleshoot issues.
2. Watch: Demo: Create a Data Action - Use data actions and identify their requirements and intended use cases.
3. BYOL Data Shares in Data Cloud: Quick Look - Share data in Data Cloud using BYOL data shares.
4. Knowledge Check: Act on Data - Check your understanding of data activations by completing this knowledge check.


Get Certification Ready
Study and prepare for the Data Cloud Consultant certification.

Cert Prep: Data Cloud Consultant
Study resources for the Data Cloud Consultant exam.


>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>


Notes--

Learn About Data Cloud Core Capabilities  - ~2 hrs 10 mins . 9 Steps
Note- Explore Data Cloud features, use cases, and best practices.


 1. unlock your data with data cloud - Complete this trail to gain the skills you need to become proficient with Data Cloud.
 2. Salesforce Data Cloud Quick look - Discover Data Cloud, the real-time platform for customer magic.
 3. Data Cloud use cases - Get the most out of Data Cloud for Sales, Service, Marketing, and more.
 4. Data Cloud powered experiences - Use Data Cloud to create personalized, real-time experiences across Salesforce and beyond.
 5. Data spaces in data cloud: quick look - Learn the benefits of using data spaces in Data Cloud.
 6. Data cloud basics for marketers - Discover the customer data platform for marketers and prepare for implementation.
 7. Data cloud for commerece quick look - Learn how to personalize the shopper experience with Data Cloud.
 8. Ethical Data Use in Personalization - Apply ethical use best practices to deliver responsible personalization solutions.
 9. Knowledge Check: Data Cloud Core Capabilities - Check your understanding Data Cloud core capabilities by completing this knowledge check.



Set Up and Administer Data Cloud
Note- Learn about the setup and administration tools for Data Cloud.

1. Data Cloud Setup - Set up your account and connect data sources to support your company’s business needs.




2. Watch: Data Cloud for Admins - Explore special considerations for setting up Data Cloud. Get tips for packaging, managing, and troubleshooting your Data Cloud instance.


>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
3. Create a Data Stream in Data Cloud - Connect your Salesforce data in Data Cloud.


Review Data Terminology
Data Stream: A data source ingested into Data Cloud.
Process of continuously collecting, processing, and storing data from various sources in a cloud-based environment.

A data stream in this context is a continuous flow of data generated from various sources
> IoT devices (sensors, smart devices)
> Social media platforms (tweets, posts)
> Financial transactions (stock prices, trades)
> Application logs (user activity, system events)

A data stream is a continuous flow of data that is generated and transmitted in real-time or near-real-time.
It can consist of a sequence of data points or records that are produced by various sources, such as sensors, applications, or user interactions.
Data streams are often characterized by their high velocity, volume, and variety, making them suitable for processing and analysis in real-time.


Key features of data streams include:

> Continuous Flow: Data is generated and sent in a steady stream rather than in discrete batches.

> Real-Time Processing: Data streams are often processed in real-time to enable immediate insights and actions.

> Time-Series Data: Many data streams consist of time-stamped data points, allowing for analysis over time.

> Dynamic Nature: The content and structure of data streams can change over time, requiring flexible processing techniques.

> Applications: Data streams are used in various applications, including financial transactions, social media feeds, IoT sensor data, and monitoring systems.

Technologies and frameworks such as Apache Kafka, Apache Flink, and Apache Spark Streaming are commonly used to handle and process data streams effectively.

1. Data Stream Definition
A data stream in this context is a continuous flow of data generated from various sources, such as:

IoT devices (sensors, smart devices)
Social media platforms (tweets, posts)
Financial transactions (stock prices, trades)
Application logs (user activity, system events)

2. Ingestion Process
Ingesting a data stream into a Data Cloud involves several steps:

Data Collection: Data is collected from the source in real-time or near-real-time. This can be done using APIs, webhooks, or direct connections to data-producing systems.
Data Transformation: As data is ingested, it may need to be transformed or enriched. This can include filtering, aggregating, or formatting the data to make it suitable for analysis.
Data Loading: The transformed data is then loaded into the Data Cloud, where it can be stored in databases, data lakes, or other storage solutions.

3. Data Cloud
A Data Cloud refers to cloud-based platforms that provide services for storing, processing, and analyzing data. Examples include:

Cloud Data Warehouses: Such as Google BigQuery, Amazon Redshift, or Snowflake, which are optimized for analytical queries.
Cloud Data Lakes: Such as Amazon S3 or Azure Data Lake Storage, which can store large volumes of unstructured or semi-structured data.
Real-Time Processing Frameworks: Such as Apache Kafka, Apache Flink, or AWS Kinesis, which can process data streams in real-time.





Data Lake Object (DLO): A storage container for the data ingested into data streams.

A Data Lake Object (DLO) refers to a specific storage container within a data lake architecture that is designed to hold the data ingested from various data streams. 
Data lakes are a type of storage repository that can hold vast amounts of raw data in its native format until it is needed for analysis. 
A data lake is a centralized repository that allows organizations to store structured, semi-structured, and unstructured data at scale.
Unlike traditional databases, which require data to be structured before storage, data lakes can accommodate data in its raw form, making them highly flexible for various types of data.
A Data Lake Object (DLO) is essentially a unit of storage within a data lake that contains ingested data. 

It can represent various types of data, including:
> Files: Such as CSV, JSON, Parquet, or Avro files that contain raw data.
> Binary Objects: Such as images, videos, or audio files.
> Structured Data: Tables or datasets that may have been transformed or organized for specific analytical purposes.

Characteristics of Data Lake Objects
 > Raw Data Storage: DLOs can store data in its original format, allowing for flexibility in how the data can be used later.
 > Schema-on-Read: Unlike traditional databases that enforce a schema on write, data lakes allow for a schema to be applied 
   when the data is read, enabling more dynamic data exploration and analysis.
 > Scalability: DLOs can scale to accommodate large volumes of data, making them suitable for big data applications.
 > Diverse Data Types: DLOs can store a wide variety of data types, including text, images, logs, and more.

Ingestion into Data Lake Objects
When data is ingested from data streams into a data lake, it is typically organized into DLOs based on certain criteria, such as:
 > Source: Data from different streams may be stored in separate DLOs for easier management.
 > Data Type: Different types of data (e.g., structured vs. unstructured) may be organized into different DLOs.
 > Time Period: Data can be partitioned into DLOs based on time intervals (e.g., daily, monthly) for easier retrieval and 
   analysis.


Benefits of Using Data Lake Objects
 > Cost-Effective Storage: Data lakes often use cheaper storage solutions, allowing organizations to store large amounts of 
   data without incurring high costs.
 > Flexibility: DLOs allow organizations to store data without needing to define its structure upfront, making it easier to 
   adapt to changing data needs.
 > Data Accessibility: DLOs can be accessed by various analytics tools and frameworks, enabling data scientists and analysts to 
   perform complex queries and analysis.

Use Cases
Data Archiving: Storing historical data for compliance or future analysis.
Machine Learning: Providing raw data for training machine learning models.
Business Intelligence: Enabling data analysts to explore and visualize data for insights.

In summary, a Data Lake Object (DLO) serves as a fundamental building block within a data lake, providing a flexible and scalable way to store and manage the diverse data ingested from various data streams. 
This architecture supports a wide range of analytical and operational use cases, making it a valuable component of modern data management strategies.


Data Model Object (DMO): A Data Cloud object created from data streams, insights, and other sources.

A Data Model Object (DMO) is a structured representation of data that is created within a Data Cloud environment, typically derived from various data streams, insights, and other data sources. DMOs are essential for organizing, analyzing, and deriving insights from data in a meaningful way. Here’s a detailed explanation of what a Data Model Object is and its significance:

1. Data Model Overview
A data model defines how data is structured, organized, and related within a database or data storage system. It provides a framework for understanding the data and its relationships, which is crucial for effective data analysis and reporting.

2. Data Model Object (DMO) Definition
A Data Model Object (DMO) is a specific instance of a data model that encapsulates the structure, relationships, and constraints of data derived from various sources, including:

Data Streams: Continuous flows of data from real-time sources, such as IoT devices, social media, or transaction logs.
Insights: Analytical results or derived metrics that provide valuable information about the data.
Other Sources: Additional data repositories, such as databases, data lakes, or external APIs.

3. Characteristics of Data Model Objects
> Structured Representation: DMOs provide a structured way to represent data, including entities, attributes, and relationships.
> Schema Definition: DMOs define the schema, which includes data types, constraints, and relationships between different data 
  entities.
> Derived Insights: DMOs can incorporate insights derived from data analysis, such as aggregations, calculations, or machine 
  learning model outputs.
> Interoperability: DMOs can be designed to work with various data processing and analytics tools, facilitating data 
  integration and analysis.

4. Creation of Data Model Objects
The creation of DMOs typically involves several steps:
> Data Ingestion: Data is ingested from various sources, including data streams and other repositories.
> Data Transformation: The ingested data may undergo transformation processes to clean, aggregate, or enrich it.
> Model Design: A data model is designed based on the requirements of the analysis, defining entities, attributes, and 
  relationships.
> DML Creation: The structured data model is instantiated as a DMO, which can then be used for querying, reporting, and 
 analysis.

5. Benefits of Using Data Model Objects
> Enhanced Data Understanding: DMOs provide a clear structure that helps users understand the data and its relationships, 
  making it easier to derive insights.
> Improved Data Quality: By defining constraints and relationships, DMOs can help ensure data integrity and quality.
> Facilitated Analytics: DMOs enable more efficient querying and analysis, as they provide a structured framework for accessing 
  and manipulating data.
> Reusability: Once created, DMOs can be reused across different analysis and applications, promoting consistency and 
  efficiency.

6. Use Cases
Business Intelligence: DMOs can be used to create dashboards and reports that visualize key performance indicators (KPIs) and other metrics.
Data Warehousing: DMOs can serve as the foundation for data warehouses, where structured data is stored for analytical purposes.
Machine Learning: DMOs can be used to prepare datasets for training machine learning models, ensuring that the data is structured and relevant.

Conclusion
In summary, a Data Model Object (DMO) is a crucial component in a Data Cloud environment that provides a structured representation of data derived from various sources, including data streams and insights. By defining the schema, relationships, and constraints of the data, DMOs facilitate effective data analysis, improve data quality, and enhance the overall understanding of the data landscape within an organization. This structured approach is essential for leveraging data effectively in decision-making and strategic planning.



Customer 360 Data Model: Data Cloud’s standard canonical data model. Data ingested into Data Cloud is mapped to DMOs found in the Customer 360 Data Model.

The Customer 360 Data Model is a way to organize and understand all the information a company has about its customers in a comprehensive and unified manner. Here’s a simple breakdown of what this means:


1. What is Customer 360?
Imagine you have a business that interacts with customers in many ways—through sales, customer service, marketing, and more. Each of these interactions generates data, like purchase history, customer support calls, feedback, and social media interactions. 
The Customer 360 concept is about bringing all this information together to create a complete picture of each customer.
This helps businesses understand their customers better and provide more personalized experiences.

2. What is a Data Model?
A data model is like a blueprint or a map that shows how different pieces of information are related to each other. 
In the case of the Customer 360 Data Model, it defines how all the customer-related data should be organized and connected. 


This includes details like:
> Customer names and contact information
> Purchase history
> Customer preferences
> Interactions with customer service


3. What is a Data Cloud?
A Data Cloud is a storage solution that allows businesses to keep all their data online, making it easy to access and analyze. It’s like having a big digital filing cabinet where you can store all your customer information securely.

4. Mapping Data to DMOs
When data is ingested into the Data Cloud, it needs to be organized according to the Customer 360 Data Model.
This process is called mapping. Here’s how it works:

> Ingesting Data: When a business collects data (like a new customer signing up or a purchase being made), this data is sent to 
  the Data Cloud.
> Mapping to DMOs: The data is then organized into specific categories defined by the Customer 360 Data Model. These categories 
  are called Data Model Objects (DMOs). For example, a new customer’s information might be mapped to a DMO that includes their 
  name, email, and purchase history.


5. Why is This Important?
> Unified View: By mapping all customer data to the Customer 360 Data Model, businesses can see a complete view of each 
  customer in one place. This helps them understand customer behavior and preferences better.
> Personalization: With a complete picture of the customer, businesses can tailor their marketing and services to meet 
  individual needs, leading to better customer satisfaction.
> Better Decision-Making: Having organized and accessible data allows businesses to make informed decisions based on real 
  insights about their customers.

Conclusion
In simple terms, the Customer 360 Data Model is a structured way to gather and organize all the information a business has about its customers in a Data Cloud. 
By mapping this data to specific categories, businesses can create a complete and clear picture of each customer, which helps them provide better services and make smarter decisions.




Starter Data Bundles: A Salesforce-defined data stream that includes mapping to the Data Cloud DMO structure.

Starter Data Bundles are a concept from Salesforce that help businesses easily get started with using data in the Salesforce Data Cloud. Here’s a simple breakdown of what this means

1. What are Starter Data Bundles?
Think of Starter Data Bundles as pre-packaged sets of data that Salesforce provides. 
These bundles contain important information that businesses often need to analyze and understand their customers better.
They are designed to make it easier for companies to start using data without having to build everything from scratch.


2. What is a Data Stream?
A data stream is like a continuous flow of information coming from different sources. For example, it could include data from customer interactions, sales transactions, or marketing campaigns. In the context of Starter Data Bundles, it means that these bundles include data that is regularly updated and relevant to the business.

3. Mapping to Data Cloud DMO Structure
When we say that Starter Data Bundles include mapping to the Data Cloud DMO structure, it means that the data in these bundles is organized in a specific way that fits into the overall data framework of the Salesforce Data Cloud. 
Here’s what that means:
> Data Organization: The data is structured according to a predefined format (called a Data Model Object or DMO) that makes it 
  easy to understand and use. This structure helps ensure that all the data fits together nicely, like pieces of a puzzle.

> Ease of Use: By having the data already mapped to this structure, businesses can quickly start analyzing it without needing 
  to spend time figuring out how to organize it themselves.

4. Why is This Important?
> Quick Start: Starter Data Bundles allow businesses to quickly access and use important data without needing extensive setup 
  or technical expertise.

> Consistency: Since the data is organized in a standard way, it helps ensure that everyone in the organization is looking at 
  the same information, which leads to better decision-making.

> Focus on Insights: With the data already structured and ready to go, businesses can focus more on analyzing the data and 
  gaining insights rather than spending time on data preparation.

Conclusion
In simple terms, Starter Data Bundles are ready-to-use sets of data provided by Salesforce that help businesses quickly access and analyze important customer information. They come with a predefined structure that makes it easy to understand and use the data, allowing companies to focus on gaining insights and making informed decisions.





>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
4. Packaging and Data Kits in Data Cloud - Learn how to create and package components in Data Cloud.
5. Data Cloud Solutions on AppExchange: Quick Look - Expand the power of Salesforce with Data Cloud solutions on AppExchange.
6. Set Up and Administer Data Cloud - Knowledge check


---------------------------------------------------------------------------------------------------------------------------------------
Connect and Model Your Data - ~2 hrs 11 mins • 6 Steps
Note - Connect various data sources and map your data.

1. Ingestion and Modeling in Data Cloud - Connect your data sources and define their relationship in Data Cloud.

Prepare to Build Your Data Model
Learning Objectives
After completing this unit, you’ll be able to:

Define key terms related to data ingestion and modeling.
Identify the benefits of using Data Cloud.

Plan Ahead
It’s amazing how much data one person can generate. And how many different sources that data can come. Even a simple shopping trip can generate customer data related to sales messages, web traffic, purchases, preferences, location, and a multitude of other sources. As the data-aware specialist, you need to keep all that information organized and accessible so you can gain a more complete understanding of your customers. It’s your responsibility to make sure the data is in the right location and to create all the necessary linkage to complete your business analytics tasks.

Defining your data model can be complex. You need to understand what data is collected (and how), the existing data structure, and how that data relates to other sources. And you need to bring all of that data together into a single, actionable view of your customer. Data Cloud gives you all the tools you need to create that single view, and then engage your customers.

Before you dig into Data Cloud, do yourself a favor and grab a piece of paper, a notebook, a whiteboard, or whatever you want to doodle on. Map out a matrix of all your data sets in columns, then create rows for special considerations for each data set. 

Data Sets

When establishing the columns of your matrix, consider these factors.

Take inventory on all the data sources you might want to incorporate:
Traditional software
External databases
CRM
Ecommerce
Data lakes
Marketing and email databases
Customer service
Digital engagement data (including web and mobile)
Analytics
Identify all data sets required for each data source, such as ecommerce data with data sets for sales order details and sales order header data.
Special Considerations

When establishing the rows of your matrix, consider these characteristics. 

Understand the primary key (the value that uniquely identifies a row of data) of each data set.
Identify any foreign keys in the data set. These ancillary keys in the source may link to the primary key of a different data set. (For example, the sales order details data set contains a product ID that corresponds to the item purchased. This product ID links to a whole separate table with more details about that product, such as color or size. The instance of product ID on the sales order details data set is the foreign key, and the instance of product ID on the product data set is the primary key.)
Determine if the data is immutable (not subject to change once a record is sent) or if the data set needs to accommodate updates to existing records.
Determine if there are any transformations you would like to apply to the data. (For example, you can use simple formulas to clean up names or perform row-based calculations.)
Review the attributes, or fields, coming from each data source. If the same field is tracked across multiple sources, decide which data source is most trusted. You can set an ordered preference of sources later on.
Make sure you have the authentication details handy to access each data set.
Take note of how often the data gets updated.
From Planning Your Concepts to Building Your Model
Now that you’ve done the legwork to understand your end-to-end implementation, the rest of the work is just mechanics. As you can see in the diagram, we take a two-phased approach to bringing in data. 

Data ingestion: Bring in all fields from a data set exactly as they are without modification. That way, you can always revert back to the original shape of the data should you make a mistake or change business requirements during setup. You can also extend the data set by creating additional formula fields for the purpose of cleaning nomenclature or performing row-based calculations. Each data set is going to be represented by a data stream in Data Cloud.
Data modeling: Map the data streams to the data model in order to create a harmonized view across sources.
As you complete this first step of ingesting each data set, refer back to your matrix and take a look at what you determined was the source of the data set. Within Data Cloud, there is a place to write in the name of the source. Next to the source, you specify the data set that you’re bringing in from that source by filling out the Object Label and Object API Name. 

Refer to the primary key of the data set in your matrix and designate that field as the primary key when defining the data source object. Did you want to enhance the data set with any additional formula fields? This is the phase where you can apply formula logic. Remember when you indicated if the data is immutable or not? Data sets with event dateTime values may be good candidates for the category of type engagement in our system. Such behavioral data sets are organized into date-based containers. When Data Cloud reads the data later on to give you segmentation counts, it knows exactly where to look in order to retrieve the information quickly.

Customer 360 Data Model
Now imagine that all your data is ingested and each data set is speaking its own language. How do we get them all to understand one another? These data sets must all conform to the same universal language in order to begin interacting with one another. That’s where the second phase, data modeling, comes in. Data Cloud utilizes a data model known as the Customer 360 Data Model. 

The model consists of several objects covering a number of subject areas. Those subject areas include (but aren’t limited to) Party, Product, Sales Order, and Engagement. The model is extensible, meaning that standard objects can have custom attributes added to them and new custom objects can be created with input from you on how that new object relates to other existing objects.

Note
Remember the foreign key designations you made in the matrix during the planning phase? You use this knowledge to extend the original footprint of the Customer 360 Data Model with your own custom objects.

You can think of the Customer 360 Data Model as a way to assign a semantic context to your source objects that everybody can understand. For example, whether you call your home a flat, a house, or a condo, you can agree that it’s a place where a person lives. Similarly, you are creating a harmonized data layer for all data sources that is abstracted away from the underlying source objects. Consider offline orders data with an order field of Salescheck_Number and online orders data with an order field of Order_ID. While these fields are distinctly named, they are both references to an Order ID. So simply tag them both in mapping as the Customer 360 Data Model reference order ID. This process removes the information from the context of the various sources and arrives at a new data layer that conforms to a single taxonomy. No matter where it comes from, your data is standardized and usable within your tasks. And that is how we make all data sets speak the same language.

Note
Learn more about the standard data model in the badge Customer 360 Data Model for Data Cloud.

Resources
Salesforce Help: Data Ingestion and Modeling
Trailhead: Customer 360 Data Model for Data Cloud
Salesforce Help: Formula Expression Library

---------------------------------------------------------------------------------------------------------------------------------------


2. Watch: Data Ingestion and Mapping - Prepare and organize data for mapping in Data Cloud.

Review Data Ingestion and Modeling Phases
Learning Objectives
After completing this unit, you’ll be able to:

Examine how data is ingested into Data Cloud.
Configure key qualifiers to help interpret ingested data.
Apply basic data modeling concepts to your account.











3. Watch: Demo: Ingestion - Explore data streams and available ingestion methods, ingest from Salesforce Sales Bundle, and map contacts from CRM.
4. Watch: Demo: Mapped Data Model Object Relationships - Explore graph view for the data model, orphan objects, and create relationships between Data Model Objects (DMOs).
5. Customer 360 Data Model for Data Cloud - Explore and use data model objects in Data Cloud.
6. Streaming Data Transforms in Data Cloud: Quick Look - Enable customers to clean and prepare data as it enters the system in real time.
7. Batch Data Transforms in Data Cloud: Quick Look - Create and schedule complex data transformations using batch data transforms.
8. Knowledge Check: Connect and Model Your Data - Check your understanding of data connection and modeling by completing this knowledge check.

-------------------------------------------------------------------------------------------------------------------------------------
Resolve Identity  ~1 hr 19 mins • 4 Steps
Note- Explore the identity resolution process in Data Cloud.

1. Data and Identity in Data Cloud - Learn the fundamentals of data mapping, identity resolution, and unified records.
2. Watch: Identity Resolution - Learn about identity resolution and Unified Profiles.
3. Quick Start: Create an Identity Resolution Ruleset - Use a ruleset to guide how Data Cloud unifies your customer profile data.
4. Resolve Identity - Knowledge check

Segment and Gain Insight    - ~3 hrs 45 mins • 9 Steps
Note - Configure segmentation and get insights from your data.

1. Quick Start: Enhance Data with Insights - Set up calculated insights with data from Data Cloud.
2. Quick Start: Create a Data Cloud Segment - Learn how to create and activate a segment in Data Cloud.
3. Segmentation and Activation - Create, filter, and activate marketing segments with Data Cloud.
4. Quick Start: Enhance Data with Insights - Set up calculated insights with data from Data Cloud.
5. Data Cloud Insights - Use your data effectively by creating calculated and real-time insights.
6. Data Cloud Insights Using SQL - Use SQL to create calculated and streaming insights in Data Cloud.
7. Insights Builder in Data Cloud - Use Insights Builder to create insights using a drag-and-drop visual builder.
8. Connect Data Cloud to Copilot and Prompt Builder - Combine harmonized data and generative AI to create powerful interactions and automations.
9. Knowledge Check: Segment and Gain Insight - Check your understanding of segmentation and insights in Data Cloud by completing this knowledge check.

Act on Data --- ~38 mins • 4 Steps
Note- Learn how to use activations and troubleshoot common issues.

1. Watch: Act on Data - Learn about activations and their use cases. Use related attributes, analyze timing dependencies, and troubleshoot issues.
2. Watch: Demo: Create a Data Action - Use data actions and identify their requirements and intended use cases.
3. BYOL Data Shares in Data Cloud: Quick Look - Share data in Data Cloud using BYOL data shares.
4. Knowledge Check: Act on Data - Check your understanding of data activations by completing this knowledge check.


Get Certification Ready
Study and prepare for the Data Cloud Consultant certification.

Cert Prep: Data Cloud Consultant
Study resources for the Data Cloud Consultant exam.




























































































































































































































