

sandbox url - https://orgfarm-700c078e12-dev-ed.develop.my.salesforce.com/?ec=301&startURL=%2Fvisualforce%2Fsession%3Furl%3Dhttps%253A%252F%252Forgfarm-700c078e12-dev-ed.develop.lightning.force.com%252Flightning%252Fpage%252Fhome

username epic.f0871736144509279@orgfarm.th

Learn About Data Cloud Core Capabilities  - ~2 hrs 10 mins . 9 Steps
Note- Explore Data Cloud features, use cases, and best practices.

 1. unlock your data with data cloud - Complete this trail to gain the skills you need to become proficient with Data Cloud.
 2. Salesforce Data Cloud Quick look - Discover Data Cloud, the real-time platform for customer magic.
 3. Data Cloud use cases - Get the most out of Data Cloud for Sales, Service, Marketing, and more.
 4. Data Cloud powered experiences - Use Data Cloud to create personalized, real-time experiences across Salesforce and beyond.
 5. Data spaces in data cloud: quick look - Learn the benefits of using data spaces in Data Cloud.
 6. Data cloud basics for marketers - Discover the customer data platform for marketers and prepare for implementation.
 7. Data cloud for commerece quick look - Learn how to personalize the shopper experience with Data Cloud.
 8. Ethical Data Use in Personalization - Apply ethical use best practices to deliver responsible personalization solutions.
 9. Knowledge Check: Data Cloud Core Capabilities - Check your understanding Data Cloud core capabilities by completing this knowledge check.



Set Up and Administer Data Cloud
Note- Learn about the setup and administration tools for Data Cloud.

1. Data Cloud Setup - Set up your account and connect data sources to support your company’s business needs.
2. Watch: Data Cloud for Admins - Explore special considerations for setting up Data Cloud. Get tips for packaging, managing, and troubleshooting your Data Cloud instance.
3. Create a Data Stream in Data Cloud - Connect your Salesforce data in Data Cloud.
4. Packaging and Data Kits in Data Cloud - Learn how to create and package components in Data Cloud.
5. Data Cloud Solutions on AppExchange: Quick Look - Expand the power of Salesforce with Data Cloud solutions on AppExchange.
6. Set Up and Administer Data Cloud - Knowledge check

Connect and Model Your Data - ~2 hrs 11 mins • 6 Steps
Note - Connect various data sources and map your data.

1. Ingestion and Modeling in Data Cloud - Connect your data sources and define their relationship in Data Cloud.
2. Watch: Data Ingestion and Mapping - Prepare and organize data for mapping in Data Cloud.
3. Watch: Demo: Ingestion - Explore data streams and available ingestion methods, ingest from Salesforce Sales Bundle, and map contacts from CRM.
4. Watch: Demo: Mapped Data Model Object Relationships - Explore graph view for the data model, orphan objects, and create relationships between Data Model Objects (DMOs).
5. Customer 360 Data Model for Data Cloud - Explore and use data model objects in Data Cloud.
6. Streaming Data Transforms in Data Cloud: Quick Look - Enable customers to clean and prepare data as it enters the system in real time.
7. Batch Data Transforms in Data Cloud: Quick Look - Create and schedule complex data transformations using batch data transforms.
8. Knowledge Check: Connect and Model Your Data - Check your understanding of data connection and modeling by completing this knowledge check.


Resolve Identity  ~1 hr 19 mins • 4 Steps
Note- Explore the identity resolution process in Data Cloud.

1. Data and Identity in Data Cloud - Learn the fundamentals of data mapping, identity resolution, and unified records.
2. Watch: Identity Resolution - Learn about identity resolution and Unified Profiles.
3. Quick Start: Create an Identity Resolution Ruleset - Use a ruleset to guide how Data Cloud unifies your customer profile data.
4. Resolve Identity - Knowledge check

Segment and Gain Insight    - ~3 hrs 45 mins • 9 Steps
Note - Configure segmentation and get insights from your data.

1. Quick Start: Enhance Data with Insights - Set up calculated insights with data from Data Cloud.
2. Quick Start: Create a Data Cloud Segment - Learn how to create and activate a segment in Data Cloud.
3. Segmentation and Activation - Create, filter, and activate marketing segments with Data Cloud.
4. Quick Start: Enhance Data with Insights - Set up calculated insights with data from Data Cloud.
5. Data Cloud Insights - Use your data effectively by creating calculated and real-time insights.
6. Data Cloud Insights Using SQL - Use SQL to create calculated and streaming insights in Data Cloud.
7. Insights Builder in Data Cloud - Use Insights Builder to create insights using a drag-and-drop visual builder.
8. Connect Data Cloud to Copilot and Prompt Builder - Combine harmonized data and generative AI to create powerful interactions and automations.
9. Knowledge Check: Segment and Gain Insight - Check your understanding of segmentation and insights in Data Cloud by completing this knowledge check.

Act on Data --- ~38 mins • 4 Steps
Note- Learn how to use activations and troubleshoot common issues.

1. Watch: Act on Data - Learn about activations and their use cases. Use related attributes, analyze timing dependencies, and troubleshoot issues.
2. Watch: Demo: Create a Data Action - Use data actions and identify their requirements and intended use cases.
3. BYOL Data Shares in Data Cloud: Quick Look - Share data in Data Cloud using BYOL data shares.
4. Knowledge Check: Act on Data - Check your understanding of data activations by completing this knowledge check.


Get Certification Ready
Study and prepare for the Data Cloud Consultant certification.

Cert Prep: Data Cloud Consultant
Study resources for the Data Cloud Consultant exam.


>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>


Notes--

Learn About Data Cloud Core Capabilities  - ~2 hrs 10 mins . 9 Steps
Note- Explore Data Cloud features, use cases, and best practices.


 1. unlock your data with data cloud - Complete this trail to gain the skills you need to become proficient with Data Cloud.
 2. Salesforce Data Cloud Quick look - Discover Data Cloud, the real-time platform for customer magic.
 3. Data Cloud use cases - Get the most out of Data Cloud for Sales, Service, Marketing, and more.
 4. Data Cloud powered experiences - Use Data Cloud to create personalized, real-time experiences across Salesforce and beyond.
 5. Data spaces in data cloud: quick look - Learn the benefits of using data spaces in Data Cloud.
 6. Data cloud basics for marketers - Discover the customer data platform for marketers and prepare for implementation.
 7. Data cloud for commerece quick look - Learn how to personalize the shopper experience with Data Cloud.
 8. Ethical Data Use in Personalization - Apply ethical use best practices to deliver responsible personalization solutions.
 9. Knowledge Check: Data Cloud Core Capabilities - Check your understanding Data Cloud core capabilities by completing this knowledge check.



Set Up and Administer Data Cloud
Note- Learn about the setup and administration tools for Data Cloud.

1. Data Cloud Setup - Set up your account and connect data sources to support your company’s business needs.




2. Watch: Data Cloud for Admins - Explore special considerations for setting up Data Cloud. Get tips for packaging, managing, and troubleshooting your Data Cloud instance.

You need to be an admin with proper access to data cloud in order to perform these tasks
Feature - Data Cloud Setup
Permission Set - Data Cloud Admin, Data Cloud Marketing Admin
Not needed - Data Cloud User, Data Cloud Data Aware Specialist, Data Cloud Marketing Manager, Data Cloud Marketing Specialist

Data Cloud is built on top of the salesforce platform, so you can take advantage of core salesforce org capabilities like
reports, dashboards, flows, authentication, application's objects.
Additionally it offers the flexibility to utilize APIs and establish integrations with other salesforce products. 

Note- Data Cloud stores ingested data in a data lake outside of the core CRM. It doesn't store that data in the 
standard salesforce database.

Before you setup data cloud it is helpful to have a look at the architecture and data residency requirements

^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
              Flow Automation
              Einstein Intelligence
                    Metadata
Transactional Database----------Data cloud

                  Hyperforce
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

                         Salesforce.org                 Existing customer data org
                      Sales           Service
                     Experience         APP
                      OMS              Loyalty

                          Data Cloud
                             |
                             |
                           Data cloud Data lake  -- ingested data persisted off- core
 Note- 
1. Data Ingestion :  Data is brought into the data cloud in its original form and stored in a data lake object (DLO)
2. Data Federation : Connect directly to data sources using Zero-ETL, avoiding data copying.

^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Your options are to provision data cloud in an existing salesforce org or to provision a new seperate home org.
provisioning data cloud in an existing salesforce org makes sense when your company has maybe a single line of business then 
you have a single salesforce org.
And you primary use cases require out of the box Data cloud Lightning Web Components LWCs and search capabilities for service
agents.

When provisioning data cloud in an existing org, keep in mind that a single data cloud instance can still connect to multiple
different salesforce orgs if you need to.
The Data Lake will still separately persist and ingest data off-core. In other words not in the salesforce database.
Data cloud still requires API access to S-objects from within this existing org because it replicates data to the Data Lake.
When you migrate org data or perhaps refactor your object model, you may need to reimplement or rework data cloud.

^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Salesforce.org
Data Cloud
   |
   |

Data cloud data lake  ---- ingested data persisted off-core

^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Deploying a brand new salesforce org to house data cloud makes sense when your maybe your company has multiple salesforce orgs
so you can dedicate one to Data cloud or if you have a highly complex enterprise architecture.

Your data cloud administrators are different from your salesforce administrators or perhaps your existing data org is highly 
customized.You just don't want to mess with it.
When deploying data cloud in a brand new org, keep in mind that you probably need to build custom LWCs in the new org to 
provide users with data cloud data views.

Data cloud can still connect to multiple salesforce core orgs but only one Enterprise Edition Marketing cloud org.

Let's review a few best practices for successful packaging, management and troubleshooting of your data cloud instance.
Data cloud supports both unmanaged and managed packaging.
For managed packages, we recommend you use versioning and push upgrades. use package versions to upgrade the components in 
newer package versions without breaking functionality for existing users.
use the push upgrade feature to automatically upgrade to a newer version of the package ensuring that all of your orgs
are on the same or latest version of your package.
You should use a package to distribute data cloud information to other orgs.
Currently, Data cloud supports the packaging of data streams, data models and calculated insights.
You should also consider using data kits which help you streamline the package creation and installation process.

^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Developer org  ------ Start > Create And Map CRM data streams > Create new data kit > Add CRM Data Stream and Data Model

                                                                                       Package kit will suggest
                                                                                       related feature meta data

                        >  Create And Upload Managed/Unmanaged Package ---------
                                                                                |
                     ------------------------------------------------------------
                     |                                                                             

Subscriber org ---- Install managed/unmanaged package > Create CRM data Stream from Data kit > Data models mapped automatically

                       > Test and Validate data against Segment

^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

When you create a salesforce package, it automatically includes all of the related meta data or none of it.
This approach is a simple way to ensure platform referential integrity is intact.
However this all or nothing approach can introduce a lot of dulplicated metadata or cause certain functionalities to fail due to
lack of related metadata.
Because of this, there are important reasons why you might consider using Data Kits for data cloud.
Firstly you deploy updates with confidence, minimizing human errors, Then you streamline deployments to deliver a complete 
solution and help you realize increased business value. 
Check out our packaging and data kits in data cloud module in trailhead for detailed information on creating, publishing, 
and installing data kits. 

Data Clous is now supported in sandbox environments as an open beta. You can use sandboxes for developemnt testing and training 
in an isolated environment. Ensuring your production data and applications remain unaffected. 

Diagnose and explore data using profile explorer, data explorer and the APIs.
With Lightning App builder, admins can customize and organize their data cloud, unified profile record pages to quickly surface
insights most relevant to their teams. Each unified individual has one page available for customization and it is viewable 
by all user groups.
Currently the page customization is orgwide.
All the users in the org see the same layout.
The Profile Explorer record page within data cloud can be customized with App builder to display records associated 
with a unified profile.


Use data explorer within the data cloud to view and validate the data that exists in your data model objects- your DMOs, 
your data lake objects your DLOs and calculated insights.

You use data explorer to ensure your data, formula and other transformations are accurate. You can also use filters to include
or exclude records as you see it.
In the CRM org where data cloud is provisioned, you can use data cloud components on the contact record. The contact record page
can be customized with data cloud profile engagements, Data cloud profile insights and data cloud profile related records.

Sharing rules are used to extend sharing access to users in public groups, roles or territories for your data cloud objects.
The objects currently supported include data streams, Calculated insights, segments, activations and activation targets

Sharing rules in salesforce consist of three main components:
> Defining the records to be shared based upon ownership and criteria
> specifying the user or group by role, territory or public group to share with
> and setting the access level to either read only or read write permissions

When you publish a segment a segment membership DMO is automatically generated.
This DMO enables you to verify the accuracy of test data within your creator segment and to gain detailed insights into the 
makeup of your segment's members.

Track the profiles that have joined or left the segment over time and view segment membership details when examining specific
profiles.

Data cloud allows you to use various tools and products to analyze our data including Tableau, CRM analytics and 
Marketing intelligence.
Analytice reports and dashboards are used for data that lives in data cloud including the DLOs, the data lake objects, the DMOs 
the data model objects and calculated insights.

Administrator reports and dashboards are built on data cloud configuration objects so you can visualize your use of data cloud
The following objects are currently supported on Lightning Report Builder:
> Data Stream,
> Segment, 
> activation targets
> and identity resolution. 

To create a report on a data cloud object you will need to configure a custom report type. And once that custom report type
is created, it will be available to you in Lightning Report Builder.
Lightning Reports are created to help you understand your data such as Data cloud segments by their status and their 
population count, publish status and publish schedule. 

   





>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
3. Create a Data Stream in Data Cloud - Connect your Salesforce data in Data Cloud.


Review Data Terminology
Data Stream: A data source ingested into Data Cloud.
Process of continuously collecting, processing, and storing data from various sources in a cloud-based environment.

A data stream in this context is a continuous flow of data generated from various sources
> IoT devices (sensors, smart devices)
> Social media platforms (tweets, posts)
> Financial transactions (stock prices, trades)
> Application logs (user activity, system events)

A data stream is a continuous flow of data that is generated and transmitted in real-time or near-real-time.
It can consist of a sequence of data points or records that are produced by various sources, such as sensors, applications, or user interactions.
Data streams are often characterized by their high velocity, volume, and variety, making them suitable for processing and analysis in real-time.


Key features of data streams include:

> Continuous Flow: Data is generated and sent in a steady stream rather than in discrete batches.

> Real-Time Processing: Data streams are often processed in real-time to enable immediate insights and actions.

> Time-Series Data: Many data streams consist of time-stamped data points, allowing for analysis over time.

> Dynamic Nature: The content and structure of data streams can change over time, requiring flexible processing techniques.

> Applications: Data streams are used in various applications, including financial transactions, social media feeds, IoT sensor data, and monitoring systems.

Technologies and frameworks such as Apache Kafka, Apache Flink, and Apache Spark Streaming are commonly used to handle and process data streams effectively.

1. Data Stream Definition
A data stream in this context is a continuous flow of data generated from various sources, such as:

IoT devices (sensors, smart devices)
Social media platforms (tweets, posts)
Financial transactions (stock prices, trades)
Application logs (user activity, system events)

2. Ingestion Process
Ingesting a data stream into a Data Cloud involves several steps:

Data Collection: Data is collected from the source in real-time or near-real-time. This can be done using APIs, webhooks, or direct connections to data-producing systems.
Data Transformation: As data is ingested, it may need to be transformed or enriched. This can include filtering, aggregating, or formatting the data to make it suitable for analysis.
Data Loading: The transformed data is then loaded into the Data Cloud, where it can be stored in databases, data lakes, or other storage solutions.

3. Data Cloud
A Data Cloud refers to cloud-based platforms that provide services for storing, processing, and analyzing data. Examples include:

Cloud Data Warehouses: Such as Google BigQuery, Amazon Redshift, or Snowflake, which are optimized for analytical queries.
Cloud Data Lakes: Such as Amazon S3 or Azure Data Lake Storage, which can store large volumes of unstructured or semi-structured data.
Real-Time Processing Frameworks: Such as Apache Kafka, Apache Flink, or AWS Kinesis, which can process data streams in real-time.





Data Lake Object (DLO): A storage container for the data ingested into data streams.

A Data Lake Object (DLO) refers to a specific storage container within a data lake architecture that is designed to hold the data ingested from various data streams. 
Data lakes are a type of storage repository that can hold vast amounts of raw data in its native format until it is needed for analysis. 
A data lake is a centralized repository that allows organizations to store structured, semi-structured, and unstructured data at scale.
Unlike traditional databases, which require data to be structured before storage, data lakes can accommodate data in its raw form, making them highly flexible for various types of data.
A Data Lake Object (DLO) is essentially a unit of storage within a data lake that contains ingested data. 

It can represent various types of data, including:
> Files: Such as CSV, JSON, Parquet, or Avro files that contain raw data.
> Binary Objects: Such as images, videos, or audio files.
> Structured Data: Tables or datasets that may have been transformed or organized for specific analytical purposes.

Characteristics of Data Lake Objects
 > Raw Data Storage: DLOs can store data in its original format, allowing for flexibility in how the data can be used later.
 > Schema-on-Read: Unlike traditional databases that enforce a schema on write, data lakes allow for a schema to be applied 
   when the data is read, enabling more dynamic data exploration and analysis.
 > Scalability: DLOs can scale to accommodate large volumes of data, making them suitable for big data applications.
 > Diverse Data Types: DLOs can store a wide variety of data types, including text, images, logs, and more.

Ingestion into Data Lake Objects
When data is ingested from data streams into a data lake, it is typically organized into DLOs based on certain criteria, such as:
 > Source: Data from different streams may be stored in separate DLOs for easier management.
 > Data Type: Different types of data (e.g., structured vs. unstructured) may be organized into different DLOs.
 > Time Period: Data can be partitioned into DLOs based on time intervals (e.g., daily, monthly) for easier retrieval and 
   analysis.


Benefits of Using Data Lake Objects
 > Cost-Effective Storage: Data lakes often use cheaper storage solutions, allowing organizations to store large amounts of 
   data without incurring high costs.
 > Flexibility: DLOs allow organizations to store data without needing to define its structure upfront, making it easier to 
   adapt to changing data needs.
 > Data Accessibility: DLOs can be accessed by various analytics tools and frameworks, enabling data scientists and analysts to 
   perform complex queries and analysis.

Use Cases
Data Archiving: Storing historical data for compliance or future analysis.
Machine Learning: Providing raw data for training machine learning models.
Business Intelligence: Enabling data analysts to explore and visualize data for insights.

In summary, a Data Lake Object (DLO) serves as a fundamental building block within a data lake, providing a flexible and scalable way to store and manage the diverse data ingested from various data streams. 
This architecture supports a wide range of analytical and operational use cases, making it a valuable component of modern data management strategies.


Data Model Object (DMO): A Data Cloud object created from data streams, insights, and other sources.

A Data Model Object (DMO) is a structured representation of data that is created within a Data Cloud environment, typically derived from various data streams, insights, and other data sources. DMOs are essential for organizing, analyzing, and deriving insights from data in a meaningful way. Here’s a detailed explanation of what a Data Model Object is and its significance:

1. Data Model Overview
A data model defines how data is structured, organized, and related within a database or data storage system. It provides a framework for understanding the data and its relationships, which is crucial for effective data analysis and reporting.

2. Data Model Object (DMO) Definition
A Data Model Object (DMO) is a specific instance of a data model that encapsulates the structure, relationships, and constraints of data derived from various sources, including:

Data Streams: Continuous flows of data from real-time sources, such as IoT devices, social media, or transaction logs.
Insights: Analytical results or derived metrics that provide valuable information about the data.
Other Sources: Additional data repositories, such as databases, data lakes, or external APIs.

3. Characteristics of Data Model Objects
> Structured Representation: DMOs provide a structured way to represent data, including entities, attributes, and relationships.
> Schema Definition: DMOs define the schema, which includes data types, constraints, and relationships between different data 
  entities.
> Derived Insights: DMOs can incorporate insights derived from data analysis, such as aggregations, calculations, or machine 
  learning model outputs.
> Interoperability: DMOs can be designed to work with various data processing and analytics tools, facilitating data 
  integration and analysis.

4. Creation of Data Model Objects
The creation of DMOs typically involves several steps:
> Data Ingestion: Data is ingested from various sources, including data streams and other repositories.
> Data Transformation: The ingested data may undergo transformation processes to clean, aggregate, or enrich it.
> Model Design: A data model is designed based on the requirements of the analysis, defining entities, attributes, and 
  relationships.
> DML Creation: The structured data model is instantiated as a DMO, which can then be used for querying, reporting, and 
 analysis.

5. Benefits of Using Data Model Objects
> Enhanced Data Understanding: DMOs provide a clear structure that helps users understand the data and its relationships, 
  making it easier to derive insights.
> Improved Data Quality: By defining constraints and relationships, DMOs can help ensure data integrity and quality.
> Facilitated Analytics: DMOs enable more efficient querying and analysis, as they provide a structured framework for accessing 
  and manipulating data.
> Reusability: Once created, DMOs can be reused across different analysis and applications, promoting consistency and 
  efficiency.

6. Use Cases
Business Intelligence: DMOs can be used to create dashboards and reports that visualize key performance indicators (KPIs) and other metrics.
Data Warehousing: DMOs can serve as the foundation for data warehouses, where structured data is stored for analytical purposes.
Machine Learning: DMOs can be used to prepare datasets for training machine learning models, ensuring that the data is structured and relevant.

Conclusion
In summary, a Data Model Object (DMO) is a crucial component in a Data Cloud environment that provides a structured representation of data derived from various sources, including data streams and insights. By defining the schema, relationships, and constraints of the data, DMOs facilitate effective data analysis, improve data quality, and enhance the overall understanding of the data landscape within an organization. This structured approach is essential for leveraging data effectively in decision-making and strategic planning.



Customer 360 Data Model: Data Cloud’s standard canonical data model. Data ingested into Data Cloud is mapped to DMOs found in the Customer 360 Data Model.

The Customer 360 Data Model is a way to organize and understand all the information a company has about its customers in a comprehensive and unified manner. Here’s a simple breakdown of what this means:


1. What is Customer 360?
Imagine you have a business that interacts with customers in many ways—through sales, customer service, marketing, and more. Each of these interactions generates data, like purchase history, customer support calls, feedback, and social media interactions. 
The Customer 360 concept is about bringing all this information together to create a complete picture of each customer.
This helps businesses understand their customers better and provide more personalized experiences.

2. What is a Data Model?
A data model is like a blueprint or a map that shows how different pieces of information are related to each other. 
In the case of the Customer 360 Data Model, it defines how all the customer-related data should be organized and connected. 


This includes details like:
> Customer names and contact information
> Purchase history
> Customer preferences
> Interactions with customer service


3. What is a Data Cloud?
A Data Cloud is a storage solution that allows businesses to keep all their data online, making it easy to access and analyze. It’s like having a big digital filing cabinet where you can store all your customer information securely.

4. Mapping Data to DMOs
When data is ingested into the Data Cloud, it needs to be organized according to the Customer 360 Data Model.
This process is called mapping. Here’s how it works:

> Ingesting Data: When a business collects data (like a new customer signing up or a purchase being made), this data is sent to 
  the Data Cloud.
> Mapping to DMOs: The data is then organized into specific categories defined by the Customer 360 Data Model. These categories 
  are called Data Model Objects (DMOs). For example, a new customer’s information might be mapped to a DMO that includes their 
  name, email, and purchase history.


5. Why is This Important?
> Unified View: By mapping all customer data to the Customer 360 Data Model, businesses can see a complete view of each 
  customer in one place. This helps them understand customer behavior and preferences better.
> Personalization: With a complete picture of the customer, businesses can tailor their marketing and services to meet 
  individual needs, leading to better customer satisfaction.
> Better Decision-Making: Having organized and accessible data allows businesses to make informed decisions based on real 
  insights about their customers.

Conclusion
In simple terms, the Customer 360 Data Model is a structured way to gather and organize all the information a business has about its customers in a Data Cloud. 
By mapping this data to specific categories, businesses can create a complete and clear picture of each customer, which helps them provide better services and make smarter decisions.




Starter Data Bundles: A Salesforce-defined data stream that includes mapping to the Data Cloud DMO structure.

Starter Data Bundles are a concept from Salesforce that help businesses easily get started with using data in the Salesforce Data Cloud. Here’s a simple breakdown of what this means

1. What are Starter Data Bundles?
Think of Starter Data Bundles as pre-packaged sets of data that Salesforce provides. 
These bundles contain important information that businesses often need to analyze and understand their customers better.
They are designed to make it easier for companies to start using data without having to build everything from scratch.


2. What is a Data Stream?
A data stream is like a continuous flow of information coming from different sources. For example, it could include data from customer interactions, sales transactions, or marketing campaigns. In the context of Starter Data Bundles, it means that these bundles include data that is regularly updated and relevant to the business.

3. Mapping to Data Cloud DMO Structure
When we say that Starter Data Bundles include mapping to the Data Cloud DMO structure, it means that the data in these bundles is organized in a specific way that fits into the overall data framework of the Salesforce Data Cloud. 
Here’s what that means:
> Data Organization: The data is structured according to a predefined format (called a Data Model Object or DMO) that makes it 
  easy to understand and use. This structure helps ensure that all the data fits together nicely, like pieces of a puzzle.

> Ease of Use: By having the data already mapped to this structure, businesses can quickly start analyzing it without needing 
  to spend time figuring out how to organize it themselves.

4. Why is This Important?
> Quick Start: Starter Data Bundles allow businesses to quickly access and use important data without needing extensive setup 
  or technical expertise.

> Consistency: Since the data is organized in a standard way, it helps ensure that everyone in the organization is looking at 
  the same information, which leads to better decision-making.

> Focus on Insights: With the data already structured and ready to go, businesses can focus more on analyzing the data and 
  gaining insights rather than spending time on data preparation.

Conclusion
In simple terms, Starter Data Bundles are ready-to-use sets of data provided by Salesforce that help businesses quickly access and analyze important customer information. They come with a predefined structure that makes it easy to understand and use the data, allowing companies to focus on gaining insights and making informed decisions.

Create a Data Stream

Connect Your Data Sources
Before you can ingest data into Data Cloud, an admin needs to configure any data source that you’d like to connect.
Data sources can be other Salesforce orgs, Marketing Cloud Engagement business units, external platforms, CSV files, and more! For this project, we’ve already connected a Service Cloud org to your Developer org.

Navigate Data Cloud
After logging into your Developer Org, navigate to Data Cloud from the App Launcher (grid icon) and type in Data Cloud. 

Create a Data Stream from a Data Bundle
Since an admin has set up your data sources already, you can now add data streams to Data Cloud. 

> In Data Cloud, go to the Data Streams tab and click New.
> Click Salesforce CRM under Connected Sources, and click Next.
> Note the Salesforce Org is pre-selected. Choose the Sales data bundle and click Next.
> Notice that the default Data Space is pre-selected and all the fields included in the bundle are listed. Leave the selections 
  as is, and click Next.
> Click Deploy.



Create a Formula Field
The benefit of starter data bundles is that they are pre-mapped to the Customer 360 Data Model and can be customized to meet your business needs. When you create a Data Cloud data stream, you can choose to generate more fields, called formula fields. These optional fields are helpful when you want to configure data to standardize formatting; add keys to help join and map data; or add flags for data that meets criteria, like “Has Birthday before 1991.” 

For this project, you create a formula field to identify leads that are located in the United States, based on the data found in the Country field of the data lake object (DLO). 

> From the Data Stream tab, locate and click the Lead_Home data stream that you created.
> From the Lead_Home data stream record, click the New Formula Field button.
> For Field Label, name your field Is US Based.
> Confirm the Filed API Name is Is_US_Based.
> For Formula Return Type, select Text.
> Enter this formula in the Transformation Formula field: IF(sourceField['Country'] == "USA" || sourceField['Country'] == 
  "United States", "TRUE" , "FALSE")

Validate the constructed formulas using the Tested Value panel:
> Type Japan into the country field and then click Test.
> Output should = FALSE.
> Type USAand then click Test.
> Output should = TRUE.

Once you have successful validation, click Save.
Verify Your Data Stream and Formula Field

Now that you have completed these steps in your Developer Org, click Verify Step to check your work before moving to the next step in the project: adding fields to the DMO and mapping data to the Customer 360 Data Model.  
--------------------------------------------------------------------------------------------------------------------------
Add and Map a Formula Field
Map Your Field to a Data Model Object

> Data Cloud has a lot of cool capabilities (such as identity resolution, segmentation, and creating calculated insights). 
> And to use these, you first need to map your ingested data to data model objects (DMOs). 
> When using Salesforce standard data bundles like we’re doing in this project, some standard data objects are pre-mapped for 
  you.
> Since you added a new custom formula field (and often businesses have other standard and custom objects in their data sources), these fields need to be manually mapped to the Customer 360 Data Model. 

Map those unmapped fields!
> Click the Data Streams tab and then select the data stream, Lead_Home.
> From the Data Mapping section, click Review.
> On one side of the screen are the searchable (1) Data Lake Objects (DLOs). On the other side, are the searchable (2) DMOs 
  (called Data Model entities in-app). To search for a field within a DLO use the DLO search bar (3) in this case, Lead_Home.
> Name the Field, Is US Based and select Text as the Data Type.
> Click Save.
> Now under the Data Lake Object, Lead_Home, search for the unmapped field Is US Based.
> Select Is US Based until highlighted and then under Data Model entities, scroll to the Lead section and under Unmapped select 
  the newly created Is US Based custom field.
> After a line is drawn to connect the two fields, click Save. 
Now let’s map a few other fields from the Lead_Home DLO that haven’t been mapped yet.

Under the Lead_Home DLO search box, search for the field Industry and click it.
Under Data Model entities, search for Industry. You’ll find it in the Lead section.
Map the two together by clicking on the DLO, then on the DMO.
After a line is drawn to connect the two fields, click Save.
Now locate Phone under the Lead_Home search box. Notice it is already mapped to Telephone Number. We also need to map it to Formatted E164 Phone Number.
Click on Phone to select it and then locate Formatted E164 Phone Number from the Unmapped section of Contact Point Phone.
Select Formatted E164 Phone Number.
After a line is drawn to connect the two fields, click Save & Close.
Prep for Identity Resolution
To unify profiles with Identity Resolution, some additional fields are required to be mapped. Map those fields.

​From Data Streams, select the Contact_Home data stream.
Under Data Mapping, click Review.
Following similar steps to above, map Business Phone from the Contact_Home DLO to Formatted E164 Phone Number under the Contact Point Phone entity.
Click Save & Close.
Follow these steps for the Account data stream.

From Data Streams go to the Account_Home data stream.
Map Account ID to Party under the Contact Point Address entity.
Map Account ID to Party under the Contact Point Phone entity.
Map Account Phone to Formatted E164 Phone Number under the Contact Point Phone entity.
Once done, click Save & Close.

>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
4. Packaging and Data Kits in Data Cloud - Learn how to create and package components in Data Cloud.
5. Data Cloud Solutions on AppExchange: Quick Look - Expand the power of Salesforce with Data Cloud solutions on AppExchange.
6. Set Up and Administer Data Cloud - Knowledge check


---------------------------------------------------------------------------------------------------------------------------------------
Connect and Model Your Data - ~2 hrs 11 mins • 6 Steps
Note - Connect various data sources and map your data.

1. Ingestion and Modeling in Data Cloud - Connect your data sources and define their relationship in Data Cloud.

Prepare to Build Your Data Model
Learning Objectives
After completing this unit, you’ll be able to:

Define key terms related to data ingestion and modeling.
Identify the benefits of using Data Cloud.

Plan Ahead
It’s amazing how much data one person can generate. And how many different sources that data can come. Even a simple shopping trip can generate customer data related to sales messages, web traffic, purchases, preferences, location, and a multitude of other sources. As the data-aware specialist, you need to keep all that information organized and accessible so you can gain a more complete understanding of your customers. It’s your responsibility to make sure the data is in the right location and to create all the necessary linkage to complete your business analytics tasks.

Defining your data model can be complex. You need to understand what data is collected (and how), the existing data structure, and how that data relates to other sources. And you need to bring all of that data together into a single, actionable view of your customer. Data Cloud gives you all the tools you need to create that single view, and then engage your customers.

Before you dig into Data Cloud, do yourself a favor and grab a piece of paper, a notebook, a whiteboard, or whatever you want to doodle on. Map out a matrix of all your data sets in columns, then create rows for special considerations for each data set. 

Data Sets

When establishing the columns of your matrix, consider these factors.

Take inventory on all the data sources you might want to incorporate:
Traditional software
External databases
CRM
Ecommerce
Data lakes
Marketing and email databases
Customer service
Digital engagement data (including web and mobile)
Analytics
Identify all data sets required for each data source, such as ecommerce data with data sets for sales order details and sales order header data.
Special Considerations

When establishing the rows of your matrix, consider these characteristics. 

Understand the primary key (the value that uniquely identifies a row of data) of each data set.
Identify any foreign keys in the data set. These ancillary keys in the source may link to the primary key of a different data set. (For example, the sales order details data set contains a product ID that corresponds to the item purchased. This product ID links to a whole separate table with more details about that product, such as color or size. The instance of product ID on the sales order details data set is the foreign key, and the instance of product ID on the product data set is the primary key.)
Determine if the data is immutable (not subject to change once a record is sent) or if the data set needs to accommodate updates to existing records.
Determine if there are any transformations you would like to apply to the data. (For example, you can use simple formulas to clean up names or perform row-based calculations.)
Review the attributes, or fields, coming from each data source. If the same field is tracked across multiple sources, decide which data source is most trusted. You can set an ordered preference of sources later on.
Make sure you have the authentication details handy to access each data set.
Take note of how often the data gets updated.
From Planning Your Concepts to Building Your Model
Now that you’ve done the legwork to understand your end-to-end implementation, the rest of the work is just mechanics. As you can see in the diagram, we take a two-phased approach to bringing in data. 

Data ingestion: Bring in all fields from a data set exactly as they are without modification. That way, you can always revert back to the original shape of the data should you make a mistake or change business requirements during setup. You can also extend the data set by creating additional formula fields for the purpose of cleaning nomenclature or performing row-based calculations. Each data set is going to be represented by a data stream in Data Cloud.
Data modeling: Map the data streams to the data model in order to create a harmonized view across sources.
As you complete this first step of ingesting each data set, refer back to your matrix and take a look at what you determined was the source of the data set. Within Data Cloud, there is a place to write in the name of the source. Next to the source, you specify the data set that you’re bringing in from that source by filling out the Object Label and Object API Name. 

Refer to the primary key of the data set in your matrix and designate that field as the primary key when defining the data source object. Did you want to enhance the data set with any additional formula fields? This is the phase where you can apply formula logic. Remember when you indicated if the data is immutable or not? Data sets with event dateTime values may be good candidates for the category of type engagement in our system. Such behavioral data sets are organized into date-based containers. When Data Cloud reads the data later on to give you segmentation counts, it knows exactly where to look in order to retrieve the information quickly.

Customer 360 Data Model
Now imagine that all your data is ingested and each data set is speaking its own language. How do we get them all to understand one another? These data sets must all conform to the same universal language in order to begin interacting with one another. That’s where the second phase, data modeling, comes in. Data Cloud utilizes a data model known as the Customer 360 Data Model. 

The model consists of several objects covering a number of subject areas. Those subject areas include (but aren’t limited to) Party, Product, Sales Order, and Engagement. The model is extensible, meaning that standard objects can have custom attributes added to them and new custom objects can be created with input from you on how that new object relates to other existing objects.

Note
Remember the foreign key designations you made in the matrix during the planning phase? You use this knowledge to extend the original footprint of the Customer 360 Data Model with your own custom objects.

You can think of the Customer 360 Data Model as a way to assign a semantic context to your source objects that everybody can understand. For example, whether you call your home a flat, a house, or a condo, you can agree that it’s a place where a person lives. Similarly, you are creating a harmonized data layer for all data sources that is abstracted away from the underlying source objects. Consider offline orders data with an order field of Salescheck_Number and online orders data with an order field of Order_ID. While these fields are distinctly named, they are both references to an Order ID. So simply tag them both in mapping as the Customer 360 Data Model reference order ID. This process removes the information from the context of the various sources and arrives at a new data layer that conforms to a single taxonomy. No matter where it comes from, your data is standardized and usable within your tasks. And that is how we make all data sets speak the same language.

Note
Learn more about the standard data model in the badge Customer 360 Data Model for Data Cloud.

Resources
Salesforce Help: Data Ingestion and Modeling
Trailhead: Customer 360 Data Model for Data Cloud
Salesforce Help: Formula Expression Library

---------------------------------------------------------------------------------------------------------------------------------------


2. Watch: Data Ingestion and Mapping - Prepare and organize data for mapping in Data Cloud.

Review Data Ingestion and Modeling Phases
Learning Objectives
After completing this unit, you’ll be able to:

Examine how data is ingested into Data Cloud.
Configure key qualifiers to help interpret ingested data.
Apply basic data modeling concepts to your account.











3. Watch: Demo: Ingestion - Explore data streams and available ingestion methods, ingest from Salesforce Sales Bundle, and map contacts from CRM.
4. Watch: Demo: Mapped Data Model Object Relationships - Explore graph view for the data model, orphan objects, and create relationships between Data Model Objects (DMOs).
5. Customer 360 Data Model for Data Cloud - Explore and use data model objects in Data Cloud.
6. Streaming Data Transforms in Data Cloud: Quick Look - Enable customers to clean and prepare data as it enters the system in real time.
7. Batch Data Transforms in Data Cloud: Quick Look - Create and schedule complex data transformations using batch data transforms.
8. Knowledge Check: Connect and Model Your Data - Check your understanding of data connection and modeling by completing this knowledge check.

-------------------------------------------------------------------------------------------------------------------------------------
Resolve Identity  ~1 hr 19 mins • 4 Steps
Note- Explore the identity resolution process in Data Cloud.

1. Data and Identity in Data Cloud - Learn the fundamentals of data mapping, identity resolution, and unified records.
2. Watch: Identity Resolution - Learn about identity resolution and Unified Profiles.
3. Quick Start: Create an Identity Resolution Ruleset - Use a ruleset to guide how Data Cloud unifies your customer profile data.
4. Resolve Identity - Knowledge check

Segment and Gain Insight    - ~3 hrs 45 mins • 9 Steps
Note - Configure segmentation and get insights from your data.

1. Quick Start: Enhance Data with Insights - Set up calculated insights with data from Data Cloud.
2. Quick Start: Create a Data Cloud Segment - Learn how to create and activate a segment in Data Cloud.
3. Segmentation and Activation - Create, filter, and activate marketing segments with Data Cloud.
4. Quick Start: Enhance Data with Insights - Set up calculated insights with data from Data Cloud.
5. Data Cloud Insights - Use your data effectively by creating calculated and real-time insights.
6. Data Cloud Insights Using SQL - Use SQL to create calculated and streaming insights in Data Cloud.
7. Insights Builder in Data Cloud - Use Insights Builder to create insights using a drag-and-drop visual builder.
8. Connect Data Cloud to Copilot and Prompt Builder - Combine harmonized data and generative AI to create powerful interactions and automations.
9. Knowledge Check: Segment and Gain Insight - Check your understanding of segmentation and insights in Data Cloud by completing this knowledge check.

Act on Data --- ~38 mins • 4 Steps
Note- Learn how to use activations and troubleshoot common issues.

1. Watch: Act on Data - Learn about activations and their use cases. Use related attributes, analyze timing dependencies, and troubleshoot issues.
2. Watch: Demo: Create a Data Action - Use data actions and identify their requirements and intended use cases.
3. BYOL Data Shares in Data Cloud: Quick Look - Share data in Data Cloud using BYOL data shares.
4. Knowledge Check: Act on Data - Check your understanding of data activations by completing this knowledge check.


Get Certification Ready
Study and prepare for the Data Cloud Consultant certification.

Cert Prep: Data Cloud Consultant
Study resources for the Data Cloud Consultant exam.




























































































































































































































